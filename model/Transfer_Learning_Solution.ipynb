{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Most of the time you won't want to train a whole convolutional network yourself. Modern ConvNets training on huge datasets like ImageNet take weeks on multiple GPUs. Instead, most people use a pretrained network either as a fixed feature extractor, or as an initial network to fine tune. In this notebook, you'll be using [VGGNet](https://arxiv.org/pdf/1409.1556.pdf) trained on the [ImageNet dataset](http://www.image-net.org/) as a feature extractor. Below is a diagram of the VGGNet architecture.\n",
    "\n",
    "<img src=\"assets/cnnarchitecture.jpg\" width=700px>\n",
    "\n",
    "VGGNet is great because it's simple and has great performance, coming in second in the ImageNet competition. The idea here is that we keep all the convolutional layers, but replace the final fully connected layers with our own classifier. This way we can use VGGNet as a feature extractor for our images then easily train a simple classifier on top of that. What we'll do is take the first fully connected layer with 4096 units, including thresholding with ReLUs. We can use those values as a code for each image, then build a classifier on top of those codes.\n",
    "\n",
    "You can read more about transfer learning from [the CS231n course notes](http://cs231n.github.io/transfer-learning/#tf).\n",
    "\n",
    "## Pretrained VGGNet\n",
    "\n",
    "We'll be using a pretrained network from https://github.com/machrisaa/tensorflow-vgg. \n",
    "\n",
    "This is a really nice implementation of VGGNet, quite easy to work with. The network has already been trained and the parameters are available from this link. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter file already exists!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "\n",
    "vgg_dir = 'tensorflow_vgg/'\n",
    "# Make sure vgg exists\n",
    "if not isdir(vgg_dir):\n",
    "    raise Exception(\"VGG directory doesn't exist!\")\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(vgg_dir + \"vgg16.npy\"):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='VGG16 Parameters') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://s3.amazonaws.com/content.udacity-data.com/nd101/vgg16.npy',\n",
    "            vgg_dir + 'vgg16.npy',\n",
    "            pbar.hook)\n",
    "else:\n",
    "    print(\"Parameter file already exists!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Flower power\n",
    "\n",
    "Here we'll be using VGGNet to classify images of flowers. To get the flower dataset, run the cell below. This dataset comes from the [TensorFlow inception tutorial](https://www.tensorflow.org/tutorials/image_retraining)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ConvNet Codes\n",
    "\n",
    "Below, we'll run through all the images in our dataset and get codes for each of them. That is, we'll run the images through the VGGNet convolutional layers and record the values of the first fully connected layer. We can then write these to a file for later when we build our own classifier.\n",
    "\n",
    "Here we're using the `vgg16` module from `tensorflow_vgg`. The network takes images of size $244 \\times 224 \\times 3$ as input. Then it has 5 sets of convolutional layers. The network implemented here has this structure (copied from [the source code](https://github.com/machrisaa/tensorflow-vgg/blob/master/vgg16.py):\n",
    "\n",
    "```\n",
    "self.conv1_1 = self.conv_layer(bgr, \"conv1_1\")\n",
    "self.conv1_2 = self.conv_layer(self.conv1_1, \"conv1_2\")\n",
    "self.pool1 = self.max_pool(self.conv1_2, 'pool1')\n",
    "\n",
    "self.conv2_1 = self.conv_layer(self.pool1, \"conv2_1\")\n",
    "self.conv2_2 = self.conv_layer(self.conv2_1, \"conv2_2\")\n",
    "self.pool2 = self.max_pool(self.conv2_2, 'pool2')\n",
    "\n",
    "self.conv3_1 = self.conv_layer(self.pool2, \"conv3_1\")\n",
    "self.conv3_2 = self.conv_layer(self.conv3_1, \"conv3_2\")\n",
    "self.conv3_3 = self.conv_layer(self.conv3_2, \"conv3_3\")\n",
    "self.pool3 = self.max_pool(self.conv3_3, 'pool3')\n",
    "\n",
    "self.conv4_1 = self.conv_layer(self.pool3, \"conv4_1\")\n",
    "self.conv4_2 = self.conv_layer(self.conv4_1, \"conv4_2\")\n",
    "self.conv4_3 = self.conv_layer(self.conv4_2, \"conv4_3\")\n",
    "self.pool4 = self.max_pool(self.conv4_3, 'pool4')\n",
    "\n",
    "self.conv5_1 = self.conv_layer(self.pool4, \"conv5_1\")\n",
    "self.conv5_2 = self.conv_layer(self.conv5_1, \"conv5_2\")\n",
    "self.conv5_3 = self.conv_layer(self.conv5_2, \"conv5_3\")\n",
    "self.pool5 = self.max_pool(self.conv5_3, 'pool5')\n",
    "\n",
    "self.fc6 = self.fc_layer(self.pool5, \"fc6\")\n",
    "self.relu6 = tf.nn.relu(self.fc6)\n",
    "```\n",
    "\n",
    "So what we want are the values of the first fully connected layer, after being ReLUd (`self.relu6`). To build the network, we use\n",
    "\n",
    "```\n",
    "with tf.Session() as sess:\n",
    "    vgg = vgg16.Vgg16()\n",
    "    input_ = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "    with tf.name_scope(\"content_vgg\"):\n",
    "        vgg.build(input_)\n",
    "```\n",
    "\n",
    "This creates the `vgg` object, then builds the graph with `vgg.build(input_)`. Then to get the values from the layer,\n",
    "\n",
    "```\n",
    "feed_dict = {input_: images}\n",
    "codes = sess.run(vgg.relu6, feed_dict=feed_dict)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_vgg import vgg16\n",
    "from tensorflow_vgg import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '15', '17', '6', '13', '24', '20', '8', '7', '11', '1', '9', '4', '5', '18', '16', '3', '23', '19', '12', '22', '21', '2', '10', '14']\n"
     ]
    }
   ],
   "source": [
    "classification_id = 12\n",
    "im_count_class = 2048 #2048 4096\n",
    "data_dir = '/storage/model/food_images/cid%d_max%d/' % (classification_id, im_count_class)\n",
    "contents = os.listdir(data_dir)\n",
    "classes = [each for each in contents if os.path.isdir(data_dir + each)]\n",
    "\n",
    "codes_file = 'codes_cid%d_cl%d' % (classification_id, im_count_class)\n",
    "labels_file = 'labels_cid%d_cl%d' % (classification_id, im_count_class)\n",
    "\n",
    "print (classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Below I'm running images through the VGG network in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/model/tensorflow_vgg/vgg16.npy\n",
      "npy file loaded\n",
      "build model started\n",
      "build model finished: 0s\n",
      "Starting 0 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1400 images processed\n",
      "1500 images processed\n",
      "1600 images processed\n",
      "1700 images processed\n",
      "1800 images processed\n",
      "1900 images processed\n",
      "2000 images processed\n",
      "2100 images processed\n",
      "2200 images processed\n",
      "2300 images processed\n",
      "2400 images processed\n",
      "2500 images processed\n",
      "2600 images processed\n",
      "2700 images processed\n",
      "2800 images processed\n",
      "2900 images processed\n",
      "3000 images processed\n",
      "3100 images processed\n",
      "3200 images processed\n",
      "3300 images processed\n",
      "3400 images processed\n",
      "3500 images processed\n",
      "3600 images processed\n",
      "3700 images processed\n",
      "3800 images processed\n",
      "3900 images processed\n",
      "4000 images processed\n",
      "4096 images processed\n",
      "Starting 15 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1400 images processed\n",
      "1500 images processed\n",
      "1600 images processed\n",
      "1700 images processed\n",
      "1800 images processed\n",
      "1900 images processed\n",
      "2000 images processed\n",
      "2100 images processed\n",
      "2200 images processed\n",
      "2300 images processed\n",
      "2400 images processed\n",
      "2500 images processed\n",
      "2600 images processed\n",
      "2700 images processed\n",
      "2800 images processed\n",
      "2900 images processed\n",
      "3000 images processed\n",
      "3100 images processed\n",
      "3200 images processed\n",
      "3300 images processed\n",
      "3400 images processed\n",
      "3500 images processed\n",
      "3600 images processed\n",
      "3700 images processed\n",
      "3800 images processed\n",
      "3900 images processed\n",
      "4000 images processed\n",
      "4064 images processed\n",
      "Starting 17 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1400 images processed\n",
      "1500 images processed\n",
      "1600 images processed\n",
      "1700 images processed\n",
      "1800 images processed\n",
      "1900 images processed\n",
      "2000 images processed\n",
      "2100 images processed\n",
      "2200 images processed\n",
      "2300 images processed\n",
      "2400 images processed\n",
      "2500 images processed\n",
      "2600 images processed\n",
      "2700 images processed\n",
      "2800 images processed\n",
      "2900 images processed\n",
      "3000 images processed\n",
      "3100 images processed\n",
      "3200 images processed\n",
      "3300 images processed\n",
      "3400 images processed\n",
      "3401 images processed\n",
      "Starting 6 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1400 images processed\n",
      "1500 images processed\n",
      "1600 images processed\n",
      "1700 images processed\n",
      "1800 images processed\n",
      "1866 images processed\n",
      "Starting 13 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1400 images processed\n",
      "1500 images processed\n",
      "1600 images processed\n",
      "1700 images processed\n",
      "1800 images processed\n",
      "1900 images processed\n",
      "2000 images processed\n",
      "2100 images processed\n",
      "2200 images processed\n",
      "2300 images processed\n",
      "2400 images processed\n",
      "2500 images processed\n",
      "2600 images processed\n",
      "2700 images processed\n",
      "2800 images processed\n",
      "2900 images processed\n",
      "3000 images processed\n",
      "3100 images processed\n",
      "3200 images processed\n",
      "3300 images processed\n",
      "3400 images processed\n",
      "3500 images processed\n",
      "3600 images processed\n",
      "3700 images processed\n",
      "3800 images processed\n",
      "3900 images processed\n",
      "4000 images processed\n",
      "4096 images processed\n",
      "Starting 24 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "534 images processed\n",
      "Starting 20 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1400 images processed\n",
      "1500 images processed\n",
      "1600 images processed\n",
      "1700 images processed\n",
      "1800 images processed\n",
      "1900 images processed\n",
      "2000 images processed\n",
      "2100 images processed\n",
      "2200 images processed\n",
      "2300 images processed\n",
      "2400 images processed\n",
      "2500 images processed\n",
      "2600 images processed\n",
      "2700 images processed\n",
      "2800 images processed\n",
      "2900 images processed\n",
      "3000 images processed\n",
      "3100 images processed\n",
      "3200 images processed\n",
      "3300 images processed\n",
      "3400 images processed\n",
      "3500 images processed\n",
      "3600 images processed\n",
      "3700 images processed\n",
      "3800 images processed\n",
      "3900 images processed\n",
      "4000 images processed\n",
      "4096 images processed\n",
      "Starting 8 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1336 images processed\n",
      "Starting 7 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1400 images processed\n",
      "1500 images processed\n",
      "1600 images processed\n",
      "1700 images processed\n",
      "1800 images processed\n",
      "1900 images processed\n",
      "2000 images processed\n",
      "2100 images processed\n",
      "2200 images processed\n",
      "2300 images processed\n",
      "2400 images processed\n",
      "2500 images processed\n",
      "2600 images processed\n",
      "2700 images processed\n",
      "2800 images processed\n",
      "2900 images processed\n",
      "2988 images processed\n",
      "Starting 11 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1400 images processed\n",
      "1500 images processed\n",
      "1600 images processed\n",
      "1700 images processed\n",
      "1800 images processed\n",
      "1900 images processed\n",
      "2000 images processed\n",
      "2100 images processed\n",
      "2200 images processed\n",
      "2300 images processed\n",
      "2400 images processed\n",
      "2500 images processed\n",
      "2600 images processed\n",
      "2700 images processed\n",
      "2800 images processed\n",
      "2900 images processed\n",
      "3000 images processed\n",
      "3100 images processed\n",
      "3200 images processed\n",
      "3300 images processed\n",
      "3400 images processed\n",
      "3500 images processed\n",
      "3600 images processed\n",
      "3700 images processed\n",
      "3800 images processed\n",
      "3900 images processed\n",
      "4000 images processed\n",
      "4096 images processed\n",
      "Starting 1 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1400 images processed\n",
      "1500 images processed\n",
      "1600 images processed\n",
      "1700 images processed\n",
      "1800 images processed\n",
      "1900 images processed\n",
      "2000 images processed\n",
      "2100 images processed\n",
      "2200 images processed\n",
      "2300 images processed\n",
      "2400 images processed\n",
      "2500 images processed\n",
      "2600 images processed\n",
      "2700 images processed\n",
      "2800 images processed\n",
      "2900 images processed\n",
      "3000 images processed\n",
      "3100 images processed\n",
      "3200 images processed\n",
      "3300 images processed\n",
      "3400 images processed\n",
      "3500 images processed\n",
      "3600 images processed\n",
      "3700 images processed\n",
      "3800 images processed\n",
      "3900 images processed\n",
      "4000 images processed\n",
      "4096 images processed\n",
      "Starting 9 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1400 images processed\n",
      "1500 images processed\n",
      "1600 images processed\n",
      "1700 images processed\n",
      "1800 images processed\n",
      "1900 images processed\n",
      "2000 images processed\n",
      "2100 images processed\n",
      "2200 images processed\n",
      "2300 images processed\n",
      "2400 images processed\n",
      "2500 images processed\n",
      "2600 images processed\n",
      "2700 images processed\n",
      "2800 images processed\n",
      "2900 images processed\n",
      "3000 images processed\n",
      "3100 images processed\n",
      "3200 images processed\n",
      "3300 images processed\n",
      "3400 images processed\n",
      "3500 images processed\n",
      "3600 images processed\n",
      "3700 images processed\n",
      "3800 images processed\n",
      "3900 images processed\n",
      "4000 images processed\n",
      "4096 images processed\n",
      "Starting 4 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1400 images processed\n",
      "1500 images processed\n",
      "1600 images processed\n",
      "1700 images processed\n",
      "1800 images processed\n",
      "1900 images processed\n",
      "2000 images processed\n",
      "2100 images processed\n",
      "2200 images processed\n",
      "2300 images processed\n",
      "2400 images processed\n",
      "2500 images processed\n",
      "2600 images processed\n",
      "2700 images processed\n",
      "2800 images processed\n",
      "2900 images processed\n",
      "3000 images processed\n",
      "3100 images processed\n",
      "3200 images processed\n",
      "3300 images processed\n",
      "3400 images processed\n",
      "3500 images processed\n",
      "3600 images processed\n",
      "3700 images processed\n",
      "3800 images processed\n",
      "3900 images processed\n",
      "4000 images processed\n",
      "4096 images processed\n",
      "Starting 5 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1400 images processed\n",
      "1500 images processed\n",
      "1600 images processed\n",
      "1700 images processed\n",
      "1800 images processed\n",
      "1900 images processed\n",
      "2000 images processed\n",
      "2100 images processed\n",
      "2200 images processed\n",
      "2300 images processed\n",
      "2400 images processed\n",
      "2500 images processed\n",
      "2600 images processed\n",
      "2700 images processed\n",
      "2800 images processed\n",
      "2900 images processed\n",
      "3000 images processed\n",
      "3100 images processed\n",
      "3200 images processed\n",
      "3300 images processed\n",
      "3400 images processed\n",
      "3500 images processed\n",
      "3600 images processed\n",
      "3700 images processed\n",
      "3800 images processed\n",
      "3900 images processed\n",
      "4000 images processed\n",
      "4096 images processed\n",
      "Starting 18 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1400 images processed\n",
      "1500 images processed\n",
      "1600 images processed\n",
      "1700 images processed\n",
      "1800 images processed\n",
      "1900 images processed\n",
      "2000 images processed\n",
      "2100 images processed\n",
      "2200 images processed\n",
      "2300 images processed\n",
      "2400 images processed\n",
      "2500 images processed\n",
      "2600 images processed\n",
      "2700 images processed\n",
      "2800 images processed\n",
      "2900 images processed\n",
      "3000 images processed\n",
      "3100 images processed\n",
      "3200 images processed\n",
      "3300 images processed\n",
      "3400 images processed\n",
      "3500 images processed\n",
      "3600 images processed\n",
      "3700 images processed\n",
      "3800 images processed\n",
      "3900 images processed\n",
      "4000 images processed\n",
      "4096 images processed\n",
      "Starting 16 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1400 images processed\n",
      "1500 images processed\n",
      "1600 images processed\n",
      "1700 images processed\n",
      "1800 images processed\n",
      "1900 images processed\n",
      "2000 images processed\n",
      "2100 images processed\n",
      "2200 images processed\n",
      "2300 images processed\n",
      "2400 images processed\n",
      "2500 images processed\n",
      "2600 images processed\n",
      "2700 images processed\n",
      "2800 images processed\n",
      "2900 images processed\n",
      "3000 images processed\n",
      "3100 images processed\n",
      "3200 images processed\n",
      "3300 images processed\n",
      "3400 images processed\n",
      "3500 images processed\n",
      "3600 images processed\n",
      "3700 images processed\n",
      "3800 images processed\n",
      "3900 images processed\n",
      "4000 images processed\n",
      "4096 images processed\n",
      "Starting 3 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1400 images processed\n",
      "1500 images processed\n",
      "1600 images processed\n",
      "1700 images processed\n",
      "1800 images processed\n",
      "1900 images processed\n",
      "2000 images processed\n",
      "2100 images processed\n",
      "2200 images processed\n",
      "2300 images processed\n",
      "2400 images processed\n",
      "2500 images processed\n",
      "2600 images processed\n",
      "2700 images processed\n",
      "2800 images processed\n",
      "2900 images processed\n",
      "3000 images processed\n",
      "3100 images processed\n",
      "3200 images processed\n",
      "3300 images processed\n",
      "3400 images processed\n",
      "3500 images processed\n",
      "3600 images processed\n",
      "3700 images processed\n",
      "3800 images processed\n",
      "3900 images processed\n",
      "4000 images processed\n",
      "4096 images processed\n",
      "Starting 23 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1400 images processed\n",
      "1500 images processed\n",
      "1600 images processed\n",
      "1700 images processed\n",
      "1800 images processed\n",
      "1900 images processed\n",
      "2000 images processed\n",
      "2100 images processed\n",
      "2200 images processed\n",
      "2300 images processed\n",
      "2400 images processed\n",
      "2500 images processed\n",
      "2600 images processed\n",
      "2700 images processed\n",
      "2800 images processed\n",
      "2900 images processed\n",
      "3000 images processed\n",
      "3100 images processed\n",
      "3200 images processed\n",
      "3300 images processed\n",
      "3400 images processed\n",
      "3500 images processed\n",
      "3600 images processed\n",
      "3700 images processed\n",
      "3800 images processed\n",
      "3900 images processed\n",
      "4000 images processed\n",
      "4096 images processed\n",
      "Starting 19 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1400 images processed\n",
      "1500 images processed\n",
      "1600 images processed\n",
      "1700 images processed\n",
      "1800 images processed\n",
      "1900 images processed\n",
      "2000 images processed\n",
      "2100 images processed\n",
      "2200 images processed\n",
      "2300 images processed\n",
      "2400 images processed\n",
      "2500 images processed\n",
      "2600 images processed\n",
      "2700 images processed\n",
      "2800 images processed\n",
      "2900 images processed\n",
      "3000 images processed\n",
      "3100 images processed\n",
      "3200 images processed\n",
      "3300 images processed\n",
      "3400 images processed\n",
      "3500 images processed\n",
      "3600 images processed\n",
      "3700 images processed\n",
      "3800 images processed\n",
      "3900 images processed\n",
      "4000 images processed\n",
      "4096 images processed\n",
      "Starting 12 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1400 images processed\n",
      "1500 images processed\n",
      "1600 images processed\n",
      "1700 images processed\n",
      "1800 images processed\n",
      "1900 images processed\n",
      "2000 images processed\n",
      "2100 images processed\n",
      "2200 images processed\n",
      "2300 images processed\n",
      "2400 images processed\n",
      "2500 images processed\n",
      "2600 images processed\n",
      "2700 images processed\n",
      "2800 images processed\n",
      "2900 images processed\n",
      "3000 images processed\n",
      "3100 images processed\n",
      "3200 images processed\n",
      "3300 images processed\n",
      "3400 images processed\n",
      "3500 images processed\n",
      "3600 images processed\n",
      "3700 images processed\n",
      "3800 images processed\n",
      "3900 images processed\n",
      "4000 images processed\n",
      "4096 images processed\n",
      "Starting 22 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1165 images processed\n",
      "Starting 21 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1400 images processed\n",
      "1500 images processed\n",
      "1600 images processed\n",
      "1700 images processed\n",
      "1800 images processed\n",
      "1900 images processed\n",
      "2000 images processed\n",
      "2063 images processed\n",
      "Starting 2 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1400 images processed\n",
      "1500 images processed\n",
      "1600 images processed\n",
      "1700 images processed\n",
      "1800 images processed\n",
      "1900 images processed\n",
      "2000 images processed\n",
      "2100 images processed\n",
      "2200 images processed\n",
      "2300 images processed\n",
      "2400 images processed\n",
      "2500 images processed\n",
      "2600 images processed\n",
      "2700 images processed\n",
      "2800 images processed\n",
      "2900 images processed\n",
      "3000 images processed\n",
      "3100 images processed\n",
      "3200 images processed\n",
      "3300 images processed\n",
      "3400 images processed\n",
      "3500 images processed\n",
      "3600 images processed\n",
      "3700 images processed\n",
      "3800 images processed\n",
      "3900 images processed\n",
      "4000 images processed\n",
      "4096 images processed\n",
      "Starting 10 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1400 images processed\n",
      "1500 images processed\n",
      "1600 images processed\n",
      "1700 images processed\n",
      "1800 images processed\n",
      "1900 images processed\n",
      "2000 images processed\n",
      "2100 images processed\n",
      "2200 images processed\n",
      "2300 images processed\n",
      "2400 images processed\n",
      "2500 images processed\n",
      "2600 images processed\n",
      "2700 images processed\n",
      "2800 images processed\n",
      "2900 images processed\n",
      "3000 images processed\n",
      "3100 images processed\n",
      "3200 images processed\n",
      "3300 images processed\n",
      "3400 images processed\n",
      "3500 images processed\n",
      "3600 images processed\n",
      "3700 images processed\n",
      "3800 images processed\n",
      "3900 images processed\n",
      "4000 images processed\n",
      "4096 images processed\n",
      "Starting 14 images\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1400 images processed\n",
      "1500 images processed\n",
      "1600 images processed\n",
      "1700 images processed\n",
      "1800 images processed\n",
      "1900 images processed\n",
      "2000 images processed\n",
      "2100 images processed\n",
      "2200 images processed\n",
      "2300 images processed\n",
      "2400 images processed\n",
      "2500 images processed\n",
      "2600 images processed\n",
      "2700 images processed\n",
      "2800 images processed\n",
      "2900 images processed\n",
      "3000 images processed\n",
      "3100 images processed\n",
      "3200 images processed\n",
      "3300 images processed\n",
      "3400 images processed\n",
      "3500 images processed\n",
      "3600 images processed\n",
      "3700 images processed\n",
      "3800 images processed\n",
      "3900 images processed\n",
      "4000 images processed\n",
      "4096 images processed\n"
     ]
    }
   ],
   "source": [
    "# Set the batch size higher if you can fit in in your GPU memory\n",
    "batch_size = 100\n",
    "codes_list = []\n",
    "labels = []\n",
    "batch = []\n",
    "\n",
    "codes = None\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    vgg = vgg16.Vgg16()\n",
    "    input_ = tf.placeholder(tf.float32, [None, 224, 224, 3], name='input')\n",
    "    with tf.name_scope(\"content_vgg\"):\n",
    "        vgg.build(input_)\n",
    "\n",
    "    for each in classes:\n",
    "        print(\"Starting {} images\".format(each))\n",
    "        class_path = data_dir + each\n",
    "        files = os.listdir(class_path)\n",
    "        for ii, file in enumerate(files, 1):\n",
    "            # Add images to the current batch\n",
    "            # utils.load_image crops the input images for us, from the center\n",
    "            img = utils.load_image(os.path.join(class_path, file))\n",
    "            batch.append(img.reshape((1, 224, 224, 3)))\n",
    "            labels.append(each)\n",
    "            \n",
    "            # Running the batch through the network to get the codes\n",
    "            if ii % batch_size == 0 or ii == len(files):\n",
    "                images = np.concatenate(batch)\n",
    "\n",
    "                feed_dict = {input_: images}\n",
    "                codes_batch = sess.run(vgg.relu6, feed_dict=feed_dict)\n",
    "                \n",
    "                # Here I'm building an array of the codes\n",
    "                if codes is None:\n",
    "                    codes = codes_batch\n",
    "                else:\n",
    "                    codes = np.concatenate((codes, codes_batch))\n",
    "                \n",
    "                # Reset to start building the next batch\n",
    "                batch = []\n",
    "                print('{} images processed'.format(ii))\n",
    "                \n",
    "# write codes to file\n",
    "with open(codes_file, 'w') as f:\n",
    "    codes.tofile(f)\n",
    "    \n",
    "# write labels to file\n",
    "import csv\n",
    "with open(labels_file, 'w') as f:\n",
    "    writer = csv.writer(f, delimiter='\\n')\n",
    "    writer.writerow(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/model/tensorflow_vgg/vgg16.npy\n",
      "npy file loaded\n",
      "Tensor(\"filter:0\", shape=(3, 3, 3, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "vgg = vgg16.Vgg16()\n",
    "\n",
    "print(vgg.get_conv_filter('conv1_1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Building the Classifier\n",
    "\n",
    "Now that we have codes for all the images, we can build a simple classifier on top of them. The codes behave just like normal input into a simple neural network. Below I'm going to have you do most of the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read codes and labels from file\n",
    "import csv\n",
    "\n",
    "with open(labels_file) as f:\n",
    "    reader = csv.reader(f, delimiter='\\n')\n",
    "    labels = np.array([each for each in reader if len(each) > 0]).squeeze()\n",
    "with open(codes_file) as f:\n",
    "    codes = np.fromfile(f, dtype=np.float32)\n",
    "    codes = codes.reshape((len(labels), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data prep\n",
    "\n",
    "As usual, now we need to one-hot encode our labels and create validation/test sets. First up, creating our labels!\n",
    "\n",
    "> **Exercise:** From scikit-learn, use [LabelBinarizer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html) to create one-hot encoded vectors from the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(labels)\n",
    "\n",
    "labels_vecs = lb.transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now you'll want to create your training, validation, and test sets. An important thing to note here is that our labels and data aren't randomized yet. We'll want to shuffle our data so the validation and test sets contain data from all classes. Otherwise, you could end up with testing sets that are all one class. Typically, you'll also want to make sure that each smaller set has the same the distribution of classes as it is for the whole data set. The easiest way to accomplish both these goals is to use [`StratifiedShuffleSplit`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html) from scikit-learn.\n",
    "\n",
    "You can create the splitter like so:\n",
    "```\n",
    "ss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "```\n",
    "Then split the data with \n",
    "```\n",
    "splitter = ss.split(x, y)\n",
    "```\n",
    "\n",
    "`ss.split` returns a generator of indices. You can pass the indices into the arrays to get the split sets. The fact that it's a generator means you either need to iterate over it, or use `next(splitter)` to get the indices. Be sure to read the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html) and the [user guide](http://scikit-learn.org/stable/modules/cross_validation.html#random-permutations-cross-validation-a-k-a-shuffle-split).\n",
    "\n",
    "> **Exercise:** Use StratifiedShuffleSplit to split the codes and labels into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "test_size = 0.2\n",
    "\n",
    "ss = StratifiedShuffleSplit(n_splits=1, test_size=test_size)\n",
    "\n",
    "train_idx, val_idx = next(ss.split(codes, labels_vecs))\n",
    "\n",
    "half_val_len = int(len(val_idx)/2)\n",
    "val_idx, test_idx = val_idx[:half_val_len], val_idx[half_val_len:]\n",
    "\n",
    "train_x, train_y = codes[train_idx], labels_vecs[train_idx]\n",
    "val_x, val_y = codes[val_idx], labels_vecs[val_idx]\n",
    "test_x, test_y = codes[test_idx], labels_vecs[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes (x, y): (38327, 4096) (38327, 25)\n",
      "Validation shapes (x, y): (4791, 4096) (4791, 25)\n",
      "Test shapes (x, y): (4791, 4096) (4791, 25)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shapes (x, y):\", train_x.shape, train_y.shape)\n",
    "print(\"Validation shapes (x, y):\", val_x.shape, val_y.shape)\n",
    "print(\"Test shapes (x, y):\", test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If you did it right, you should see these sizes for the training sets:\n",
    "\n",
    "```\n",
    "Train shapes (x, y): (2936, 4096) (2936, 5)\n",
    "Validation shapes (x, y): (367, 4096) (367, 5)\n",
    "Test shapes (x, y): (367, 4096) (367, 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Classifier layers\n",
    "\n",
    "Once you have the convolutional codes, you just need to build a classfier from some fully connected layers. You use the codes as the inputs and the image labels as targets. Otherwise the classifier is a typical neural network.\n",
    "\n",
    "> **Exercise:** With the codes and labels loaded, build the classifier. Consider the codes as your inputs, each of them are 4096D vectors. You'll want to use a hidden layer and an output layer as your classifier. Remember that the output layer needs to have one unit for each class and a softmax activation function. Use the cross entropy to calculate the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/model/tensorflow_vgg/vgg16.npy\n",
      "npy file loaded\n",
      "build model started\n",
      "build model finished: 0s\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    input_ = tf.placeholder(tf.float32, [None, 224, 224, 3], name='input')\n",
    "    vgg = vgg16.Vgg16()\n",
    "    vgg.build(input_)\n",
    "    \n",
    "inputs_ = vgg.relu6# tf.placeholder(tf.float32, shape=[None, codes.shape[1]], name='feature_inputs')\n",
    "labels_ = tf.placeholder(tf.int64, shape=[None, labels_vecs.shape[1]], name='labels')\n",
    "\n",
    "fc = tf.contrib.layers.fully_connected(inputs_, 256)\n",
    "logits = tf.contrib.layers.fully_connected(fc, labels_vecs.shape[1], activation_fn=None, scope='final_training_ops')\n",
    "tf.summary.histogram('pre_activations', logits)\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels_, logits=logits)\n",
    "tf.summary.histogram('activations', cross_entropy)\n",
    "\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "predicted = tf.nn.softmax(logits, name='final_result')\n",
    "correct_pred = tf.equal(tf.argmax(predicted, 1), tf.argmax(labels_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "tf.summary.scalar(\"loss\", cost)\n",
    "\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Batches!\n",
    "\n",
    "Here is just a simple way to do batches. I've written it so that it includes all the data. Sometimes you'll throw out some data at the end to make sure you have full batches. Here I just extend the last batch to include the remaining data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, n_batches=10):\n",
    "    \"\"\" Return a generator that yields batches from arrays x and y. \"\"\"\n",
    "    batch_size = len(x)//n_batches\n",
    "    \n",
    "    for ii in range(0, n_batches*batch_size, batch_size):\n",
    "        # If we're not on the last batch, grab data with size batch_size\n",
    "        if ii != (n_batches-1)*batch_size:\n",
    "            X, Y = x[ii: ii+batch_size], y[ii: ii+batch_size] \n",
    "        # On the last batch, grab the rest of the data\n",
    "        else:\n",
    "            X, Y = x[ii:], y[ii:]\n",
    "        # I love generators\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training\n",
    "\n",
    "Here, we'll train the network.\n",
    "\n",
    "> **Exercise:** So far we've been providing the training code for you. Here, I'm going to give you a bit more of a challenge and have you write the code to train the network. Of course, you'll be able to see my solution if you need help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: c12_b1_e600_i2048_t0.2_2018-03-25T21:55:36\n",
      "Epoch: 1/600 Iteration: 0 Training loss: 9.92270\n",
      "Epoch: 2/600 Iteration: 1 Training loss: 10.52784\n",
      "Epoch: 3/600 Iteration: 2 Training loss: 10.36070\n",
      "Epoch: 4/600 Iteration: 3 Training loss: 8.97594\n",
      "Epoch: 5/600 Iteration: 4 Training loss: 7.66441\n",
      "Epoch: 4/600 Iteration: 5 Validation Acc: 0.0670\n",
      "Epoch: 6/600 Iteration: 5 Training loss: 6.17654\n",
      "Epoch: 7/600 Iteration: 6 Training loss: 5.23632\n",
      "Epoch: 8/600 Iteration: 7 Training loss: 4.54708\n",
      "Epoch: 9/600 Iteration: 8 Training loss: 3.98472\n",
      "Epoch: 10/600 Iteration: 9 Training loss: 3.57511\n",
      "Epoch: 9/600 Iteration: 10 Validation Acc: 0.0701\n",
      "Epoch: 11/600 Iteration: 10 Training loss: 3.33303\n",
      "Epoch: 12/600 Iteration: 11 Training loss: 3.21787\n",
      "Epoch: 13/600 Iteration: 12 Training loss: 3.17235\n",
      "Epoch: 14/600 Iteration: 13 Training loss: 3.15531\n",
      "Epoch: 15/600 Iteration: 14 Training loss: 3.14704\n",
      "Epoch: 14/600 Iteration: 15 Validation Acc: 0.0643\n",
      "Epoch: 16/600 Iteration: 15 Training loss: 3.14074\n",
      "Epoch: 17/600 Iteration: 16 Training loss: 3.13488\n",
      "Epoch: 18/600 Iteration: 17 Training loss: 3.13064\n",
      "Epoch: 19/600 Iteration: 18 Training loss: 3.12778\n",
      "Epoch: 20/600 Iteration: 19 Training loss: 3.12434\n",
      "Epoch: 19/600 Iteration: 20 Validation Acc: 0.0768\n",
      "Epoch: 21/600 Iteration: 20 Training loss: 3.11809\n",
      "Epoch: 22/600 Iteration: 21 Training loss: 3.10923\n",
      "Epoch: 23/600 Iteration: 22 Training loss: 3.09977\n",
      "Epoch: 24/600 Iteration: 23 Training loss: 3.09136\n",
      "Epoch: 25/600 Iteration: 24 Training loss: 3.08427\n",
      "Epoch: 24/600 Iteration: 25 Validation Acc: 0.0918\n",
      "Epoch: 26/600 Iteration: 25 Training loss: 3.07801\n",
      "Epoch: 27/600 Iteration: 26 Training loss: 3.07162\n",
      "Epoch: 28/600 Iteration: 27 Training loss: 3.06439\n",
      "Epoch: 29/600 Iteration: 28 Training loss: 3.05674\n",
      "Epoch: 30/600 Iteration: 29 Training loss: 3.04888\n",
      "Epoch: 29/600 Iteration: 30 Validation Acc: 0.0927\n",
      "Epoch: 31/600 Iteration: 30 Training loss: 3.04103\n",
      "Epoch: 32/600 Iteration: 31 Training loss: 3.03348\n",
      "Epoch: 33/600 Iteration: 32 Training loss: 3.02649\n",
      "Epoch: 34/600 Iteration: 33 Training loss: 3.01931\n",
      "Epoch: 35/600 Iteration: 34 Training loss: 3.01144\n",
      "Epoch: 34/600 Iteration: 35 Validation Acc: 0.0998\n",
      "Epoch: 36/600 Iteration: 35 Training loss: 3.00312\n",
      "Epoch: 37/600 Iteration: 36 Training loss: 2.99503\n",
      "Epoch: 38/600 Iteration: 37 Training loss: 2.98744\n",
      "Epoch: 39/600 Iteration: 38 Training loss: 2.97934\n",
      "Epoch: 40/600 Iteration: 39 Training loss: 2.97119\n",
      "Epoch: 39/600 Iteration: 40 Validation Acc: 0.1121\n",
      "Epoch: 41/600 Iteration: 40 Training loss: 2.96339\n",
      "Epoch: 42/600 Iteration: 41 Training loss: 2.95514\n",
      "Epoch: 43/600 Iteration: 42 Training loss: 2.94692\n",
      "Epoch: 44/600 Iteration: 43 Training loss: 2.93882\n",
      "Epoch: 45/600 Iteration: 44 Training loss: 2.93045\n",
      "Epoch: 44/600 Iteration: 45 Validation Acc: 0.1158\n",
      "Epoch: 46/600 Iteration: 45 Training loss: 2.92225\n",
      "Epoch: 47/600 Iteration: 46 Training loss: 2.91392\n",
      "Epoch: 48/600 Iteration: 47 Training loss: 2.90552\n",
      "Epoch: 49/600 Iteration: 48 Training loss: 2.89731\n",
      "Epoch: 50/600 Iteration: 49 Training loss: 2.88863\n",
      "Epoch: 49/600 Iteration: 50 Validation Acc: 0.1238\n",
      "Epoch: 51/600 Iteration: 50 Training loss: 2.88014\n",
      "Epoch: 52/600 Iteration: 51 Training loss: 2.87122\n",
      "Epoch: 53/600 Iteration: 52 Training loss: 2.86229\n",
      "Epoch: 54/600 Iteration: 53 Training loss: 2.85321\n",
      "Epoch: 55/600 Iteration: 54 Training loss: 2.84401\n",
      "Epoch: 54/600 Iteration: 55 Validation Acc: 0.1353\n",
      "Epoch: 56/600 Iteration: 55 Training loss: 2.83479\n",
      "Epoch: 57/600 Iteration: 56 Training loss: 2.82551\n",
      "Epoch: 58/600 Iteration: 57 Training loss: 2.81619\n",
      "Epoch: 59/600 Iteration: 58 Training loss: 2.80689\n",
      "Epoch: 60/600 Iteration: 59 Training loss: 2.79754\n",
      "Epoch: 59/600 Iteration: 60 Validation Acc: 0.1426\n",
      "Epoch: 61/600 Iteration: 60 Training loss: 2.78843\n",
      "Epoch: 62/600 Iteration: 61 Training loss: 2.77918\n",
      "Epoch: 63/600 Iteration: 62 Training loss: 2.77001\n",
      "Epoch: 64/600 Iteration: 63 Training loss: 2.76069\n",
      "Epoch: 65/600 Iteration: 64 Training loss: 2.75140\n",
      "Epoch: 64/600 Iteration: 65 Validation Acc: 0.1520\n",
      "Epoch: 66/600 Iteration: 65 Training loss: 2.74188\n",
      "Epoch: 67/600 Iteration: 66 Training loss: 2.73241\n",
      "Epoch: 68/600 Iteration: 67 Training loss: 2.72304\n",
      "Epoch: 69/600 Iteration: 68 Training loss: 2.71360\n",
      "Epoch: 70/600 Iteration: 69 Training loss: 2.70415\n",
      "Epoch: 69/600 Iteration: 70 Validation Acc: 0.1551\n",
      "Epoch: 71/600 Iteration: 70 Training loss: 2.69473\n",
      "Epoch: 72/600 Iteration: 71 Training loss: 2.68537\n",
      "Epoch: 73/600 Iteration: 72 Training loss: 2.67598\n",
      "Epoch: 74/600 Iteration: 73 Training loss: 2.66658\n",
      "Epoch: 75/600 Iteration: 74 Training loss: 2.65717\n",
      "Epoch: 74/600 Iteration: 75 Validation Acc: 0.1578\n",
      "Epoch: 76/600 Iteration: 75 Training loss: 2.64785\n",
      "Epoch: 77/600 Iteration: 76 Training loss: 2.63863\n",
      "Epoch: 78/600 Iteration: 77 Training loss: 2.62957\n",
      "Epoch: 79/600 Iteration: 78 Training loss: 2.62050\n",
      "Epoch: 80/600 Iteration: 79 Training loss: 2.61136\n",
      "Epoch: 79/600 Iteration: 80 Validation Acc: 0.1691\n",
      "Epoch: 81/600 Iteration: 80 Training loss: 2.60153\n",
      "Epoch: 82/600 Iteration: 81 Training loss: 2.59220\n",
      "Epoch: 83/600 Iteration: 82 Training loss: 2.58323\n",
      "Epoch: 84/600 Iteration: 83 Training loss: 2.57396\n",
      "Epoch: 85/600 Iteration: 84 Training loss: 2.56525\n",
      "Epoch: 84/600 Iteration: 85 Validation Acc: 0.1743\n",
      "Epoch: 86/600 Iteration: 85 Training loss: 2.55612\n",
      "Epoch: 87/600 Iteration: 86 Training loss: 2.54676\n",
      "Epoch: 88/600 Iteration: 87 Training loss: 2.53720\n",
      "Epoch: 89/600 Iteration: 88 Training loss: 2.52820\n",
      "Epoch: 90/600 Iteration: 89 Training loss: 2.51923\n",
      "Epoch: 89/600 Iteration: 90 Validation Acc: 0.1810\n",
      "Epoch: 91/600 Iteration: 90 Training loss: 2.51031\n",
      "Epoch: 92/600 Iteration: 91 Training loss: 2.50116\n",
      "Epoch: 93/600 Iteration: 92 Training loss: 2.49134\n",
      "Epoch: 94/600 Iteration: 93 Training loss: 2.48197\n",
      "Epoch: 95/600 Iteration: 94 Training loss: 2.47335\n",
      "Epoch: 94/600 Iteration: 95 Validation Acc: 0.1858\n",
      "Epoch: 96/600 Iteration: 95 Training loss: 2.46458\n",
      "Epoch: 97/600 Iteration: 96 Training loss: 2.45587\n",
      "Epoch: 98/600 Iteration: 97 Training loss: 2.44655\n",
      "Epoch: 99/600 Iteration: 98 Training loss: 2.43714\n",
      "Epoch: 100/600 Iteration: 99 Training loss: 2.42791\n",
      "Epoch: 99/600 Iteration: 100 Validation Acc: 0.1924\n",
      "Epoch: 101/600 Iteration: 100 Training loss: 2.41952\n",
      "Epoch: 102/600 Iteration: 101 Training loss: 2.41099\n",
      "Epoch: 103/600 Iteration: 102 Training loss: 2.40167\n",
      "Epoch: 104/600 Iteration: 103 Training loss: 2.39263\n",
      "Epoch: 105/600 Iteration: 104 Training loss: 2.38464\n",
      "Epoch: 104/600 Iteration: 105 Validation Acc: 0.1989\n",
      "Epoch: 106/600 Iteration: 105 Training loss: 2.37532\n",
      "Epoch: 107/600 Iteration: 106 Training loss: 2.36608\n",
      "Epoch: 108/600 Iteration: 107 Training loss: 2.35739\n",
      "Epoch: 109/600 Iteration: 108 Training loss: 2.34937\n",
      "Epoch: 110/600 Iteration: 109 Training loss: 2.34131\n",
      "Epoch: 109/600 Iteration: 110 Validation Acc: 0.2043\n",
      "Epoch: 111/600 Iteration: 110 Training loss: 2.33363\n",
      "Epoch: 112/600 Iteration: 111 Training loss: 2.32613\n",
      "Epoch: 113/600 Iteration: 112 Training loss: 2.31448\n",
      "Epoch: 114/600 Iteration: 113 Training loss: 2.30947\n",
      "Epoch: 115/600 Iteration: 114 Training loss: 2.30343\n",
      "Epoch: 114/600 Iteration: 115 Validation Acc: 0.2075\n",
      "Epoch: 116/600 Iteration: 115 Training loss: 2.29267\n",
      "Epoch: 117/600 Iteration: 116 Training loss: 2.28532\n",
      "Epoch: 118/600 Iteration: 117 Training loss: 2.27588\n",
      "Epoch: 119/600 Iteration: 118 Training loss: 2.26591\n",
      "Epoch: 120/600 Iteration: 119 Training loss: 2.26130\n",
      "Epoch: 119/600 Iteration: 120 Validation Acc: 0.2131\n",
      "Epoch: 121/600 Iteration: 120 Training loss: 2.25062\n",
      "Epoch: 122/600 Iteration: 121 Training loss: 2.24433\n",
      "Epoch: 123/600 Iteration: 122 Training loss: 2.23403\n",
      "Epoch: 124/600 Iteration: 123 Training loss: 2.22956\n",
      "Epoch: 125/600 Iteration: 124 Training loss: 2.22053\n",
      "Epoch: 124/600 Iteration: 125 Validation Acc: 0.2215\n",
      "Epoch: 126/600 Iteration: 125 Training loss: 2.21208\n",
      "Epoch: 127/600 Iteration: 126 Training loss: 2.20172\n",
      "Epoch: 128/600 Iteration: 127 Training loss: 2.19537\n",
      "Epoch: 129/600 Iteration: 128 Training loss: 2.18583\n",
      "Epoch: 130/600 Iteration: 129 Training loss: 2.18100\n",
      "Epoch: 129/600 Iteration: 130 Validation Acc: 0.2313\n",
      "Epoch: 131/600 Iteration: 130 Training loss: 2.17000\n",
      "Epoch: 132/600 Iteration: 131 Training loss: 2.16422\n",
      "Epoch: 133/600 Iteration: 132 Training loss: 2.15469\n",
      "Epoch: 134/600 Iteration: 133 Training loss: 2.14793\n",
      "Epoch: 135/600 Iteration: 134 Training loss: 2.13943\n",
      "Epoch: 134/600 Iteration: 135 Validation Acc: 0.2365\n",
      "Epoch: 136/600 Iteration: 135 Training loss: 2.13248\n",
      "Epoch: 137/600 Iteration: 136 Training loss: 2.12418\n",
      "Epoch: 138/600 Iteration: 137 Training loss: 2.11745\n",
      "Epoch: 139/600 Iteration: 138 Training loss: 2.11032\n",
      "Epoch: 140/600 Iteration: 139 Training loss: 2.10621\n",
      "Epoch: 139/600 Iteration: 140 Validation Acc: 0.2413\n",
      "Epoch: 141/600 Iteration: 140 Training loss: 2.09539\n",
      "Epoch: 142/600 Iteration: 141 Training loss: 2.08718\n",
      "Epoch: 143/600 Iteration: 142 Training loss: 2.07829\n",
      "Epoch: 144/600 Iteration: 143 Training loss: 2.07265\n",
      "Epoch: 145/600 Iteration: 144 Training loss: 2.06661\n",
      "Epoch: 144/600 Iteration: 145 Validation Acc: 0.2482\n",
      "Epoch: 146/600 Iteration: 145 Training loss: 2.05928\n",
      "Epoch: 147/600 Iteration: 146 Training loss: 2.05124\n",
      "Epoch: 148/600 Iteration: 147 Training loss: 2.04522\n",
      "Epoch: 149/600 Iteration: 148 Training loss: 2.03567\n",
      "Epoch: 150/600 Iteration: 149 Training loss: 2.03071\n",
      "Epoch: 149/600 Iteration: 150 Validation Acc: 0.2536\n",
      "Epoch: 151/600 Iteration: 150 Training loss: 2.02314\n",
      "Epoch: 152/600 Iteration: 151 Training loss: 2.01643\n",
      "Epoch: 153/600 Iteration: 152 Training loss: 2.01039\n",
      "Epoch: 154/600 Iteration: 153 Training loss: 2.00474\n",
      "Epoch: 155/600 Iteration: 154 Training loss: 1.99678\n",
      "Epoch: 154/600 Iteration: 155 Validation Acc: 0.2597\n",
      "Epoch: 156/600 Iteration: 155 Training loss: 1.99241\n",
      "Epoch: 157/600 Iteration: 156 Training loss: 1.98183\n",
      "Epoch: 158/600 Iteration: 157 Training loss: 1.97726\n",
      "Epoch: 159/600 Iteration: 158 Training loss: 1.96949\n",
      "Epoch: 160/600 Iteration: 159 Training loss: 1.96084\n",
      "Epoch: 159/600 Iteration: 160 Validation Acc: 0.2655\n",
      "Epoch: 161/600 Iteration: 160 Training loss: 1.95923\n",
      "Epoch: 162/600 Iteration: 161 Training loss: 1.94818\n",
      "Epoch: 163/600 Iteration: 162 Training loss: 1.94092\n",
      "Epoch: 164/600 Iteration: 163 Training loss: 1.93679\n",
      "Epoch: 165/600 Iteration: 164 Training loss: 1.92626\n",
      "Epoch: 164/600 Iteration: 165 Validation Acc: 0.2703\n",
      "Epoch: 166/600 Iteration: 165 Training loss: 1.92351\n",
      "Epoch: 167/600 Iteration: 166 Training loss: 1.91407\n",
      "Epoch: 168/600 Iteration: 167 Training loss: 1.90721\n",
      "Epoch: 169/600 Iteration: 168 Training loss: 1.90149\n",
      "Epoch: 170/600 Iteration: 169 Training loss: 1.89631\n",
      "Epoch: 169/600 Iteration: 170 Validation Acc: 0.2780\n",
      "Epoch: 171/600 Iteration: 170 Training loss: 1.88817\n",
      "Epoch: 172/600 Iteration: 171 Training loss: 1.88193\n",
      "Epoch: 173/600 Iteration: 172 Training loss: 1.87752\n",
      "Epoch: 174/600 Iteration: 173 Training loss: 1.86981\n",
      "Epoch: 175/600 Iteration: 174 Training loss: 1.86354\n",
      "Epoch: 174/600 Iteration: 175 Validation Acc: 0.2814\n",
      "Epoch: 176/600 Iteration: 175 Training loss: 1.85803\n",
      "Epoch: 177/600 Iteration: 176 Training loss: 1.84719\n",
      "Epoch: 178/600 Iteration: 177 Training loss: 1.84248\n",
      "Epoch: 179/600 Iteration: 178 Training loss: 1.83635\n",
      "Epoch: 180/600 Iteration: 179 Training loss: 1.82614\n",
      "Epoch: 179/600 Iteration: 180 Validation Acc: 0.2874\n",
      "Epoch: 181/600 Iteration: 180 Training loss: 1.82247\n",
      "Epoch: 182/600 Iteration: 181 Training loss: 1.81303\n",
      "Epoch: 183/600 Iteration: 182 Training loss: 1.80781\n",
      "Epoch: 184/600 Iteration: 183 Training loss: 1.80097\n",
      "Epoch: 185/600 Iteration: 184 Training loss: 1.79471\n",
      "Epoch: 184/600 Iteration: 185 Validation Acc: 0.2918\n",
      "Epoch: 186/600 Iteration: 185 Training loss: 1.78881\n",
      "Epoch: 187/600 Iteration: 186 Training loss: 1.78339\n",
      "Epoch: 188/600 Iteration: 187 Training loss: 1.77741\n",
      "Epoch: 189/600 Iteration: 188 Training loss: 1.76984\n",
      "Epoch: 190/600 Iteration: 189 Training loss: 1.76309\n",
      "Epoch: 189/600 Iteration: 190 Validation Acc: 0.2926\n",
      "Epoch: 191/600 Iteration: 190 Training loss: 1.75789\n",
      "Epoch: 192/600 Iteration: 191 Training loss: 1.74881\n",
      "Epoch: 193/600 Iteration: 192 Training loss: 1.74434\n",
      "Epoch: 194/600 Iteration: 193 Training loss: 1.73692\n",
      "Epoch: 195/600 Iteration: 194 Training loss: 1.73198\n",
      "Epoch: 194/600 Iteration: 195 Validation Acc: 0.3012\n",
      "Epoch: 196/600 Iteration: 195 Training loss: 1.72686\n",
      "Epoch: 197/600 Iteration: 196 Training loss: 1.72167\n",
      "Epoch: 198/600 Iteration: 197 Training loss: 1.71497\n",
      "Epoch: 199/600 Iteration: 198 Training loss: 1.70612\n",
      "Epoch: 200/600 Iteration: 199 Training loss: 1.70004\n",
      "Epoch: 199/600 Iteration: 200 Validation Acc: 0.3075\n",
      "Epoch: 201/600 Iteration: 200 Training loss: 1.69585\n",
      "Epoch: 202/600 Iteration: 201 Training loss: 1.68971\n",
      "Epoch: 203/600 Iteration: 202 Training loss: 1.68577\n",
      "Epoch: 204/600 Iteration: 203 Training loss: 1.68327\n",
      "Epoch: 205/600 Iteration: 204 Training loss: 1.67218\n",
      "Epoch: 204/600 Iteration: 205 Validation Acc: 0.3102\n",
      "Epoch: 206/600 Iteration: 205 Training loss: 1.66730\n",
      "Epoch: 207/600 Iteration: 206 Training loss: 1.66840\n",
      "Epoch: 208/600 Iteration: 207 Training loss: 1.66056\n",
      "Epoch: 209/600 Iteration: 208 Training loss: 1.65366\n",
      "Epoch: 210/600 Iteration: 209 Training loss: 1.64564\n",
      "Epoch: 209/600 Iteration: 210 Validation Acc: 0.3104\n",
      "Epoch: 211/600 Iteration: 210 Training loss: 1.64605\n",
      "Epoch: 212/600 Iteration: 211 Training loss: 1.63576\n",
      "Epoch: 213/600 Iteration: 212 Training loss: 1.62731\n",
      "Epoch: 214/600 Iteration: 213 Training loss: 1.63077\n",
      "Epoch: 215/600 Iteration: 214 Training loss: 1.62986\n",
      "Epoch: 214/600 Iteration: 215 Validation Acc: 0.3239\n",
      "Epoch: 216/600 Iteration: 215 Training loss: 1.60972\n",
      "Epoch: 217/600 Iteration: 216 Training loss: 1.62200\n",
      "Epoch: 218/600 Iteration: 217 Training loss: 1.60183\n",
      "Epoch: 219/600 Iteration: 218 Training loss: 1.60231\n",
      "Epoch: 220/600 Iteration: 219 Training loss: 1.59661\n",
      "Epoch: 219/600 Iteration: 220 Validation Acc: 0.3269\n",
      "Epoch: 221/600 Iteration: 220 Training loss: 1.58653\n",
      "Epoch: 222/600 Iteration: 221 Training loss: 1.57993\n",
      "Epoch: 223/600 Iteration: 222 Training loss: 1.57575\n",
      "Epoch: 224/600 Iteration: 223 Training loss: 1.56543\n",
      "Epoch: 225/600 Iteration: 224 Training loss: 1.56254\n",
      "Epoch: 224/600 Iteration: 225 Validation Acc: 0.3277\n",
      "Epoch: 226/600 Iteration: 225 Training loss: 1.55786\n",
      "Epoch: 227/600 Iteration: 226 Training loss: 1.54823\n",
      "Epoch: 228/600 Iteration: 227 Training loss: 1.54782\n",
      "Epoch: 229/600 Iteration: 228 Training loss: 1.53774\n",
      "Epoch: 230/600 Iteration: 229 Training loss: 1.53204\n",
      "Epoch: 229/600 Iteration: 230 Validation Acc: 0.3358\n",
      "Epoch: 231/600 Iteration: 230 Training loss: 1.52600\n",
      "Epoch: 232/600 Iteration: 231 Training loss: 1.52187\n",
      "Epoch: 233/600 Iteration: 232 Training loss: 1.51360\n",
      "Epoch: 234/600 Iteration: 233 Training loss: 1.51199\n",
      "Epoch: 235/600 Iteration: 234 Training loss: 1.50249\n",
      "Epoch: 234/600 Iteration: 235 Validation Acc: 0.3404\n",
      "Epoch: 236/600 Iteration: 235 Training loss: 1.49884\n",
      "Epoch: 237/600 Iteration: 236 Training loss: 1.49399\n",
      "Epoch: 238/600 Iteration: 237 Training loss: 1.48519\n",
      "Epoch: 239/600 Iteration: 238 Training loss: 1.48291\n",
      "Epoch: 240/600 Iteration: 239 Training loss: 1.47565\n",
      "Epoch: 239/600 Iteration: 240 Validation Acc: 0.3459\n",
      "Epoch: 241/600 Iteration: 240 Training loss: 1.46879\n",
      "Epoch: 242/600 Iteration: 241 Training loss: 1.46545\n",
      "Epoch: 243/600 Iteration: 242 Training loss: 1.45847\n",
      "Epoch: 244/600 Iteration: 243 Training loss: 1.45524\n",
      "Epoch: 245/600 Iteration: 244 Training loss: 1.44828\n",
      "Epoch: 244/600 Iteration: 245 Validation Acc: 0.3490\n",
      "Epoch: 246/600 Iteration: 245 Training loss: 1.44333\n",
      "Epoch: 247/600 Iteration: 246 Training loss: 1.43863\n",
      "Epoch: 248/600 Iteration: 247 Training loss: 1.43151\n",
      "Epoch: 249/600 Iteration: 248 Training loss: 1.42784\n",
      "Epoch: 250/600 Iteration: 249 Training loss: 1.42267\n",
      "Epoch: 249/600 Iteration: 250 Validation Acc: 0.3550\n",
      "Epoch: 251/600 Iteration: 250 Training loss: 1.41656\n",
      "Epoch: 252/600 Iteration: 251 Training loss: 1.41361\n",
      "Epoch: 253/600 Iteration: 252 Training loss: 1.41169\n",
      "Epoch: 254/600 Iteration: 253 Training loss: 1.40731\n",
      "Epoch: 255/600 Iteration: 254 Training loss: 1.40410\n",
      "Epoch: 254/600 Iteration: 255 Validation Acc: 0.3578\n",
      "Epoch: 256/600 Iteration: 255 Training loss: 1.39810\n",
      "Epoch: 257/600 Iteration: 256 Training loss: 1.39371\n",
      "Epoch: 258/600 Iteration: 257 Training loss: 1.38901\n",
      "Epoch: 259/600 Iteration: 258 Training loss: 1.38890\n",
      "Epoch: 260/600 Iteration: 259 Training loss: 1.38297\n",
      "Epoch: 259/600 Iteration: 260 Validation Acc: 0.3573\n",
      "Epoch: 261/600 Iteration: 260 Training loss: 1.37728\n",
      "Epoch: 262/600 Iteration: 261 Training loss: 1.37426\n",
      "Epoch: 263/600 Iteration: 262 Training loss: 1.36435\n",
      "Epoch: 264/600 Iteration: 263 Training loss: 1.36933\n",
      "Epoch: 265/600 Iteration: 264 Training loss: 1.35629\n",
      "Epoch: 264/600 Iteration: 265 Validation Acc: 0.3617\n",
      "Epoch: 266/600 Iteration: 265 Training loss: 1.35327\n",
      "Epoch: 267/600 Iteration: 266 Training loss: 1.34516\n",
      "Epoch: 268/600 Iteration: 267 Training loss: 1.34006\n",
      "Epoch: 269/600 Iteration: 268 Training loss: 1.33745\n",
      "Epoch: 270/600 Iteration: 269 Training loss: 1.33243\n",
      "Epoch: 269/600 Iteration: 270 Validation Acc: 0.3651\n",
      "Epoch: 271/600 Iteration: 270 Training loss: 1.32575\n",
      "Epoch: 272/600 Iteration: 271 Training loss: 1.32470\n",
      "Epoch: 273/600 Iteration: 272 Training loss: 1.31397\n",
      "Epoch: 274/600 Iteration: 273 Training loss: 1.31647\n",
      "Epoch: 275/600 Iteration: 274 Training loss: 1.30771\n",
      "Epoch: 274/600 Iteration: 275 Validation Acc: 0.3657\n",
      "Epoch: 276/600 Iteration: 275 Training loss: 1.30520\n",
      "Epoch: 277/600 Iteration: 276 Training loss: 1.29803\n",
      "Epoch: 278/600 Iteration: 277 Training loss: 1.29463\n",
      "Epoch: 279/600 Iteration: 278 Training loss: 1.29023\n",
      "Epoch: 280/600 Iteration: 279 Training loss: 1.28611\n",
      "Epoch: 279/600 Iteration: 280 Validation Acc: 0.3719\n",
      "Epoch: 281/600 Iteration: 280 Training loss: 1.28119\n",
      "Epoch: 282/600 Iteration: 281 Training loss: 1.27750\n",
      "Epoch: 283/600 Iteration: 282 Training loss: 1.27259\n",
      "Epoch: 284/600 Iteration: 283 Training loss: 1.27155\n",
      "Epoch: 285/600 Iteration: 284 Training loss: 1.26458\n",
      "Epoch: 284/600 Iteration: 285 Validation Acc: 0.3755\n",
      "Epoch: 286/600 Iteration: 285 Training loss: 1.26242\n",
      "Epoch: 287/600 Iteration: 286 Training loss: 1.25759\n",
      "Epoch: 288/600 Iteration: 287 Training loss: 1.25778\n",
      "Epoch: 289/600 Iteration: 288 Training loss: 1.25055\n",
      "Epoch: 290/600 Iteration: 289 Training loss: 1.24569\n",
      "Epoch: 289/600 Iteration: 290 Validation Acc: 0.3767\n",
      "Epoch: 291/600 Iteration: 290 Training loss: 1.24625\n",
      "Epoch: 292/600 Iteration: 291 Training loss: 1.23462\n",
      "Epoch: 293/600 Iteration: 292 Training loss: 1.23229\n",
      "Epoch: 294/600 Iteration: 293 Training loss: 1.22784\n",
      "Epoch: 295/600 Iteration: 294 Training loss: 1.22227\n",
      "Epoch: 294/600 Iteration: 295 Validation Acc: 0.3795\n",
      "Epoch: 296/600 Iteration: 295 Training loss: 1.22053\n",
      "Epoch: 297/600 Iteration: 296 Training loss: 1.21300\n",
      "Epoch: 298/600 Iteration: 297 Training loss: 1.21182\n",
      "Epoch: 299/600 Iteration: 298 Training loss: 1.20290\n",
      "Epoch: 300/600 Iteration: 299 Training loss: 1.20267\n",
      "Epoch: 299/600 Iteration: 300 Validation Acc: 0.3811\n",
      "Epoch: 301/600 Iteration: 300 Training loss: 1.19486\n",
      "Epoch: 302/600 Iteration: 301 Training loss: 1.19475\n",
      "Epoch: 303/600 Iteration: 302 Training loss: 1.18627\n",
      "Epoch: 304/600 Iteration: 303 Training loss: 1.18161\n",
      "Epoch: 305/600 Iteration: 304 Training loss: 1.17957\n",
      "Epoch: 304/600 Iteration: 305 Validation Acc: 0.3922\n",
      "Epoch: 306/600 Iteration: 305 Training loss: 1.17281\n",
      "Epoch: 307/600 Iteration: 306 Training loss: 1.17046\n",
      "Epoch: 308/600 Iteration: 307 Training loss: 1.16639\n",
      "Epoch: 309/600 Iteration: 308 Training loss: 1.16249\n",
      "Epoch: 310/600 Iteration: 309 Training loss: 1.15881\n",
      "Epoch: 309/600 Iteration: 310 Validation Acc: 0.3918\n",
      "Epoch: 311/600 Iteration: 310 Training loss: 1.15371\n",
      "Epoch: 312/600 Iteration: 311 Training loss: 1.14941\n",
      "Epoch: 313/600 Iteration: 312 Training loss: 1.14640\n",
      "Epoch: 314/600 Iteration: 313 Training loss: 1.14196\n",
      "Epoch: 315/600 Iteration: 314 Training loss: 1.14236\n",
      "Epoch: 314/600 Iteration: 315 Validation Acc: 0.3953\n",
      "Epoch: 316/600 Iteration: 315 Training loss: 1.13611\n",
      "Epoch: 317/600 Iteration: 316 Training loss: 1.13324\n",
      "Epoch: 318/600 Iteration: 317 Training loss: 1.13175\n",
      "Epoch: 319/600 Iteration: 318 Training loss: 1.12660\n",
      "Epoch: 320/600 Iteration: 319 Training loss: 1.12284\n",
      "Epoch: 319/600 Iteration: 320 Validation Acc: 0.4022\n",
      "Epoch: 321/600 Iteration: 320 Training loss: 1.12045\n",
      "Epoch: 322/600 Iteration: 321 Training loss: 1.11680\n",
      "Epoch: 323/600 Iteration: 322 Training loss: 1.11567\n",
      "Epoch: 324/600 Iteration: 323 Training loss: 1.10933\n",
      "Epoch: 325/600 Iteration: 324 Training loss: 1.10853\n",
      "Epoch: 324/600 Iteration: 325 Validation Acc: 0.4012\n",
      "Epoch: 326/600 Iteration: 325 Training loss: 1.10246\n",
      "Epoch: 327/600 Iteration: 326 Training loss: 1.09743\n",
      "Epoch: 328/600 Iteration: 327 Training loss: 1.09836\n",
      "Epoch: 329/600 Iteration: 328 Training loss: 1.08923\n",
      "Epoch: 330/600 Iteration: 329 Training loss: 1.09085\n",
      "Epoch: 329/600 Iteration: 330 Validation Acc: 0.4078\n",
      "Epoch: 331/600 Iteration: 330 Training loss: 1.08321\n",
      "Epoch: 332/600 Iteration: 331 Training loss: 1.08142\n",
      "Epoch: 333/600 Iteration: 332 Training loss: 1.07589\n",
      "Epoch: 334/600 Iteration: 333 Training loss: 1.07441\n",
      "Epoch: 335/600 Iteration: 334 Training loss: 1.07094\n",
      "Epoch: 334/600 Iteration: 335 Validation Acc: 0.4112\n",
      "Epoch: 336/600 Iteration: 335 Training loss: 1.06460\n",
      "Epoch: 337/600 Iteration: 336 Training loss: 1.06450\n",
      "Epoch: 338/600 Iteration: 337 Training loss: 1.06011\n",
      "Epoch: 339/600 Iteration: 338 Training loss: 1.06139\n",
      "Epoch: 340/600 Iteration: 339 Training loss: 1.05432\n",
      "Epoch: 339/600 Iteration: 340 Validation Acc: 0.4118\n",
      "Epoch: 341/600 Iteration: 340 Training loss: 1.05712\n",
      "Epoch: 342/600 Iteration: 341 Training loss: 1.04381\n",
      "Epoch: 343/600 Iteration: 342 Training loss: 1.04760\n",
      "Epoch: 344/600 Iteration: 343 Training loss: 1.04255\n",
      "Epoch: 345/600 Iteration: 344 Training loss: 1.03608\n",
      "Epoch: 344/600 Iteration: 345 Validation Acc: 0.4152\n",
      "Epoch: 346/600 Iteration: 345 Training loss: 1.03510\n",
      "Epoch: 347/600 Iteration: 346 Training loss: 1.03375\n",
      "Epoch: 348/600 Iteration: 347 Training loss: 1.02737\n",
      "Epoch: 349/600 Iteration: 348 Training loss: 1.02780\n",
      "Epoch: 350/600 Iteration: 349 Training loss: 1.02125\n",
      "Epoch: 349/600 Iteration: 350 Validation Acc: 0.4179\n",
      "Epoch: 351/600 Iteration: 350 Training loss: 1.01756\n",
      "Epoch: 352/600 Iteration: 351 Training loss: 1.01657\n",
      "Epoch: 353/600 Iteration: 352 Training loss: 1.00726\n",
      "Epoch: 354/600 Iteration: 353 Training loss: 1.00670\n",
      "Epoch: 355/600 Iteration: 354 Training loss: 1.00394\n",
      "Epoch: 354/600 Iteration: 355 Validation Acc: 0.4223\n",
      "Epoch: 356/600 Iteration: 355 Training loss: 0.99693\n",
      "Epoch: 357/600 Iteration: 356 Training loss: 1.00015\n",
      "Epoch: 358/600 Iteration: 357 Training loss: 0.99515\n",
      "Epoch: 359/600 Iteration: 358 Training loss: 0.99134\n",
      "Epoch: 360/600 Iteration: 359 Training loss: 0.98940\n",
      "Epoch: 359/600 Iteration: 360 Validation Acc: 0.4243\n",
      "Epoch: 361/600 Iteration: 360 Training loss: 0.98535\n",
      "Epoch: 362/600 Iteration: 361 Training loss: 0.97996\n",
      "Epoch: 363/600 Iteration: 362 Training loss: 0.97646\n",
      "Epoch: 364/600 Iteration: 363 Training loss: 0.97190\n",
      "Epoch: 365/600 Iteration: 364 Training loss: 0.97312\n",
      "Epoch: 364/600 Iteration: 365 Validation Acc: 0.4279\n",
      "Epoch: 366/600 Iteration: 365 Training loss: 0.96774\n",
      "Epoch: 367/600 Iteration: 366 Training loss: 0.96366\n",
      "Epoch: 368/600 Iteration: 367 Training loss: 0.96347\n",
      "Epoch: 369/600 Iteration: 368 Training loss: 0.95468\n",
      "Epoch: 370/600 Iteration: 369 Training loss: 0.95602\n",
      "Epoch: 369/600 Iteration: 370 Validation Acc: 0.4279\n",
      "Epoch: 371/600 Iteration: 370 Training loss: 0.95120\n",
      "Epoch: 372/600 Iteration: 371 Training loss: 0.95213\n",
      "Epoch: 373/600 Iteration: 372 Training loss: 0.94678\n",
      "Epoch: 374/600 Iteration: 373 Training loss: 0.94735\n",
      "Epoch: 375/600 Iteration: 374 Training loss: 0.94002\n",
      "Epoch: 374/600 Iteration: 375 Validation Acc: 0.4298\n",
      "Epoch: 376/600 Iteration: 375 Training loss: 0.93610\n",
      "Epoch: 377/600 Iteration: 376 Training loss: 0.93556\n",
      "Epoch: 378/600 Iteration: 377 Training loss: 0.93352\n",
      "Epoch: 379/600 Iteration: 378 Training loss: 0.92656\n",
      "Epoch: 380/600 Iteration: 379 Training loss: 0.92537\n",
      "Epoch: 379/600 Iteration: 380 Validation Acc: 0.4356\n",
      "Epoch: 381/600 Iteration: 380 Training loss: 0.92574\n",
      "Epoch: 382/600 Iteration: 381 Training loss: 0.92359\n",
      "Epoch: 383/600 Iteration: 382 Training loss: 0.91795\n",
      "Epoch: 384/600 Iteration: 383 Training loss: 0.91795\n",
      "Epoch: 385/600 Iteration: 384 Training loss: 0.91325\n",
      "Epoch: 384/600 Iteration: 385 Validation Acc: 0.4341\n",
      "Epoch: 386/600 Iteration: 385 Training loss: 0.91269\n",
      "Epoch: 387/600 Iteration: 386 Training loss: 0.90883\n",
      "Epoch: 388/600 Iteration: 387 Training loss: 0.91110\n",
      "Epoch: 389/600 Iteration: 388 Training loss: 0.89966\n",
      "Epoch: 390/600 Iteration: 389 Training loss: 0.90200\n",
      "Epoch: 389/600 Iteration: 390 Validation Acc: 0.4387\n",
      "Epoch: 391/600 Iteration: 390 Training loss: 0.89331\n",
      "Epoch: 392/600 Iteration: 391 Training loss: 0.89618\n",
      "Epoch: 393/600 Iteration: 392 Training loss: 0.88629\n",
      "Epoch: 394/600 Iteration: 393 Training loss: 0.88601\n",
      "Epoch: 395/600 Iteration: 394 Training loss: 0.88602\n",
      "Epoch: 394/600 Iteration: 395 Validation Acc: 0.4385\n",
      "Epoch: 396/600 Iteration: 395 Training loss: 0.88107\n",
      "Epoch: 397/600 Iteration: 396 Training loss: 0.87621\n",
      "Epoch: 398/600 Iteration: 397 Training loss: 0.87400\n",
      "Epoch: 399/600 Iteration: 398 Training loss: 0.87207\n",
      "Epoch: 400/600 Iteration: 399 Training loss: 0.86685\n",
      "Epoch: 399/600 Iteration: 400 Validation Acc: 0.4419\n",
      "Epoch: 401/600 Iteration: 400 Training loss: 0.86538\n",
      "Epoch: 402/600 Iteration: 401 Training loss: 0.86163\n",
      "Epoch: 403/600 Iteration: 402 Training loss: 0.85958\n",
      "Epoch: 404/600 Iteration: 403 Training loss: 0.85490\n",
      "Epoch: 405/600 Iteration: 404 Training loss: 0.85344\n",
      "Epoch: 404/600 Iteration: 405 Validation Acc: 0.4463\n",
      "Epoch: 406/600 Iteration: 405 Training loss: 0.84933\n",
      "Epoch: 407/600 Iteration: 406 Training loss: 0.84772\n",
      "Epoch: 408/600 Iteration: 407 Training loss: 0.84687\n",
      "Epoch: 409/600 Iteration: 408 Training loss: 0.84218\n",
      "Epoch: 410/600 Iteration: 409 Training loss: 0.84127\n",
      "Epoch: 409/600 Iteration: 410 Validation Acc: 0.4452\n",
      "Epoch: 411/600 Iteration: 410 Training loss: 0.83812\n",
      "Epoch: 412/600 Iteration: 411 Training loss: 0.83342\n",
      "Epoch: 413/600 Iteration: 412 Training loss: 0.83418\n",
      "Epoch: 414/600 Iteration: 413 Training loss: 0.82969\n",
      "Epoch: 415/600 Iteration: 414 Training loss: 0.82628\n",
      "Epoch: 414/600 Iteration: 415 Validation Acc: 0.4479\n",
      "Epoch: 416/600 Iteration: 415 Training loss: 0.82309\n",
      "Epoch: 417/600 Iteration: 416 Training loss: 0.82130\n",
      "Epoch: 418/600 Iteration: 417 Training loss: 0.81615\n",
      "Epoch: 419/600 Iteration: 418 Training loss: 0.81359\n",
      "Epoch: 420/600 Iteration: 419 Training loss: 0.81220\n",
      "Epoch: 419/600 Iteration: 420 Validation Acc: 0.4498\n",
      "Epoch: 421/600 Iteration: 420 Training loss: 0.81011\n",
      "Epoch: 422/600 Iteration: 421 Training loss: 0.80706\n",
      "Epoch: 423/600 Iteration: 422 Training loss: 0.80414\n",
      "Epoch: 424/600 Iteration: 423 Training loss: 0.80260\n",
      "Epoch: 425/600 Iteration: 424 Training loss: 0.79988\n",
      "Epoch: 424/600 Iteration: 425 Validation Acc: 0.4548\n",
      "Epoch: 426/600 Iteration: 425 Training loss: 0.79724\n",
      "Epoch: 427/600 Iteration: 426 Training loss: 0.79518\n",
      "Epoch: 428/600 Iteration: 427 Training loss: 0.79136\n",
      "Epoch: 429/600 Iteration: 428 Training loss: 0.78996\n",
      "Epoch: 430/600 Iteration: 429 Training loss: 0.78993\n",
      "Epoch: 429/600 Iteration: 430 Validation Acc: 0.4582\n",
      "Epoch: 431/600 Iteration: 430 Training loss: 0.78746\n",
      "Epoch: 432/600 Iteration: 431 Training loss: 0.78551\n",
      "Epoch: 433/600 Iteration: 432 Training loss: 0.78575\n",
      "Epoch: 434/600 Iteration: 433 Training loss: 0.78288\n",
      "Epoch: 435/600 Iteration: 434 Training loss: 0.78199\n",
      "Epoch: 434/600 Iteration: 435 Validation Acc: 0.4575\n",
      "Epoch: 436/600 Iteration: 435 Training loss: 0.77792\n",
      "Epoch: 437/600 Iteration: 436 Training loss: 0.78080\n",
      "Epoch: 438/600 Iteration: 437 Training loss: 0.77399\n",
      "Epoch: 439/600 Iteration: 438 Training loss: 0.77453\n",
      "Epoch: 440/600 Iteration: 439 Training loss: 0.77004\n",
      "Epoch: 439/600 Iteration: 440 Validation Acc: 0.4600\n",
      "Epoch: 441/600 Iteration: 440 Training loss: 0.76343\n",
      "Epoch: 442/600 Iteration: 441 Training loss: 0.76708\n",
      "Epoch: 443/600 Iteration: 442 Training loss: 0.76058\n",
      "Epoch: 444/600 Iteration: 443 Training loss: 0.75697\n",
      "Epoch: 445/600 Iteration: 444 Training loss: 0.75799\n",
      "Epoch: 444/600 Iteration: 445 Validation Acc: 0.4630\n",
      "Epoch: 446/600 Iteration: 445 Training loss: 0.75068\n",
      "Epoch: 447/600 Iteration: 446 Training loss: 0.75181\n",
      "Epoch: 448/600 Iteration: 447 Training loss: 0.74816\n",
      "Epoch: 449/600 Iteration: 448 Training loss: 0.74770\n",
      "Epoch: 450/600 Iteration: 449 Training loss: 0.74688\n",
      "Epoch: 449/600 Iteration: 450 Validation Acc: 0.4638\n",
      "Epoch: 451/600 Iteration: 450 Training loss: 0.74344\n",
      "Epoch: 452/600 Iteration: 451 Training loss: 0.74143\n",
      "Epoch: 453/600 Iteration: 452 Training loss: 0.74169\n",
      "Epoch: 454/600 Iteration: 453 Training loss: 0.73902\n",
      "Epoch: 455/600 Iteration: 454 Training loss: 0.73887\n",
      "Epoch: 454/600 Iteration: 455 Validation Acc: 0.4673\n",
      "Epoch: 456/600 Iteration: 455 Training loss: 0.73378\n",
      "Epoch: 457/600 Iteration: 456 Training loss: 0.73206\n",
      "Epoch: 458/600 Iteration: 457 Training loss: 0.73566\n",
      "Epoch: 459/600 Iteration: 458 Training loss: 0.72866\n",
      "Epoch: 460/600 Iteration: 459 Training loss: 0.72842\n",
      "Epoch: 459/600 Iteration: 460 Validation Acc: 0.4661\n",
      "Epoch: 461/600 Iteration: 460 Training loss: 0.72591\n",
      "Epoch: 462/600 Iteration: 461 Training loss: 0.72278\n",
      "Epoch: 463/600 Iteration: 462 Training loss: 0.71671\n",
      "Epoch: 464/600 Iteration: 463 Training loss: 0.71344\n",
      "Epoch: 465/600 Iteration: 464 Training loss: 0.71359\n",
      "Epoch: 464/600 Iteration: 465 Validation Acc: 0.4713\n",
      "Epoch: 466/600 Iteration: 465 Training loss: 0.71053\n",
      "Epoch: 467/600 Iteration: 466 Training loss: 0.70851\n",
      "Epoch: 468/600 Iteration: 467 Training loss: 0.70525\n",
      "Epoch: 469/600 Iteration: 468 Training loss: 0.70452\n",
      "Epoch: 470/600 Iteration: 469 Training loss: 0.70360\n",
      "Epoch: 469/600 Iteration: 470 Validation Acc: 0.4726\n",
      "Epoch: 471/600 Iteration: 470 Training loss: 0.69874\n",
      "Epoch: 472/600 Iteration: 471 Training loss: 0.69825\n",
      "Epoch: 473/600 Iteration: 472 Training loss: 0.69643\n",
      "Epoch: 474/600 Iteration: 473 Training loss: 0.69341\n",
      "Epoch: 475/600 Iteration: 474 Training loss: 0.69245\n",
      "Epoch: 474/600 Iteration: 475 Validation Acc: 0.4738\n",
      "Epoch: 476/600 Iteration: 475 Training loss: 0.68963\n",
      "Epoch: 477/600 Iteration: 476 Training loss: 0.68753\n",
      "Epoch: 478/600 Iteration: 477 Training loss: 0.68692\n",
      "Epoch: 479/600 Iteration: 478 Training loss: 0.68649\n",
      "Epoch: 480/600 Iteration: 479 Training loss: 0.68268\n",
      "Epoch: 479/600 Iteration: 480 Validation Acc: 0.4757\n",
      "Epoch: 481/600 Iteration: 480 Training loss: 0.68186\n",
      "Epoch: 482/600 Iteration: 481 Training loss: 0.67814\n",
      "Epoch: 483/600 Iteration: 482 Training loss: 0.67967\n",
      "Epoch: 484/600 Iteration: 483 Training loss: 0.67824\n",
      "Epoch: 485/600 Iteration: 484 Training loss: 0.67717\n",
      "Epoch: 484/600 Iteration: 485 Validation Acc: 0.4757\n",
      "Epoch: 486/600 Iteration: 485 Training loss: 0.67752\n",
      "Epoch: 487/600 Iteration: 486 Training loss: 0.66873\n",
      "Epoch: 488/600 Iteration: 487 Training loss: 0.66717\n",
      "Epoch: 489/600 Iteration: 488 Training loss: 0.66710\n",
      "Epoch: 490/600 Iteration: 489 Training loss: 0.66300\n",
      "Epoch: 489/600 Iteration: 490 Validation Acc: 0.4784\n",
      "Epoch: 491/600 Iteration: 490 Training loss: 0.66119\n",
      "Epoch: 492/600 Iteration: 491 Training loss: 0.65850\n",
      "Epoch: 493/600 Iteration: 492 Training loss: 0.65697\n",
      "Epoch: 494/600 Iteration: 493 Training loss: 0.65552\n",
      "Epoch: 495/600 Iteration: 494 Training loss: 0.64798\n",
      "Epoch: 494/600 Iteration: 495 Validation Acc: 0.4813\n",
      "Epoch: 496/600 Iteration: 495 Training loss: 0.64921\n",
      "Epoch: 497/600 Iteration: 496 Training loss: 0.64535\n",
      "Epoch: 498/600 Iteration: 497 Training loss: 0.64684\n",
      "Epoch: 499/600 Iteration: 498 Training loss: 0.64305\n",
      "Epoch: 500/600 Iteration: 499 Training loss: 0.64046\n",
      "Epoch: 499/600 Iteration: 500 Validation Acc: 0.4819\n",
      "Epoch: 501/600 Iteration: 500 Training loss: 0.64309\n",
      "Epoch: 502/600 Iteration: 501 Training loss: 0.63445\n",
      "Epoch: 503/600 Iteration: 502 Training loss: 0.63587\n",
      "Epoch: 504/600 Iteration: 503 Training loss: 0.62901\n",
      "Epoch: 505/600 Iteration: 504 Training loss: 0.62970\n",
      "Epoch: 504/600 Iteration: 505 Validation Acc: 0.4857\n",
      "Epoch: 506/600 Iteration: 505 Training loss: 0.62341\n",
      "Epoch: 507/600 Iteration: 506 Training loss: 0.62339\n",
      "Epoch: 508/600 Iteration: 507 Training loss: 0.62116\n",
      "Epoch: 509/600 Iteration: 508 Training loss: 0.61877\n",
      "Epoch: 510/600 Iteration: 509 Training loss: 0.61776\n",
      "Epoch: 509/600 Iteration: 510 Validation Acc: 0.4855\n",
      "Epoch: 511/600 Iteration: 510 Training loss: 0.61779\n",
      "Epoch: 512/600 Iteration: 511 Training loss: 0.61773\n",
      "Epoch: 513/600 Iteration: 512 Training loss: 0.61432\n",
      "Epoch: 514/600 Iteration: 513 Training loss: 0.61163\n",
      "Epoch: 515/600 Iteration: 514 Training loss: 0.61231\n",
      "Epoch: 514/600 Iteration: 515 Validation Acc: 0.4878\n",
      "Epoch: 516/600 Iteration: 515 Training loss: 0.60669\n",
      "Epoch: 517/600 Iteration: 516 Training loss: 0.60775\n",
      "Epoch: 518/600 Iteration: 517 Training loss: 0.60424\n",
      "Epoch: 519/600 Iteration: 518 Training loss: 0.60366\n",
      "Epoch: 520/600 Iteration: 519 Training loss: 0.59955\n",
      "Epoch: 519/600 Iteration: 520 Validation Acc: 0.4870\n",
      "Epoch: 521/600 Iteration: 520 Training loss: 0.59892\n",
      "Epoch: 522/600 Iteration: 521 Training loss: 0.60080\n",
      "Epoch: 523/600 Iteration: 522 Training loss: 0.59381\n",
      "Epoch: 524/600 Iteration: 523 Training loss: 0.59398\n",
      "Epoch: 525/600 Iteration: 524 Training loss: 0.59108\n",
      "Epoch: 524/600 Iteration: 525 Validation Acc: 0.4901\n",
      "Epoch: 526/600 Iteration: 525 Training loss: 0.59080\n",
      "Epoch: 527/600 Iteration: 526 Training loss: 0.58755\n",
      "Epoch: 528/600 Iteration: 527 Training loss: 0.58547\n",
      "Epoch: 529/600 Iteration: 528 Training loss: 0.58459\n",
      "Epoch: 530/600 Iteration: 529 Training loss: 0.58022\n",
      "Epoch: 529/600 Iteration: 530 Validation Acc: 0.4915\n",
      "Epoch: 531/600 Iteration: 530 Training loss: 0.58130\n",
      "Epoch: 532/600 Iteration: 531 Training loss: 0.57798\n",
      "Epoch: 533/600 Iteration: 532 Training loss: 0.57683\n",
      "Epoch: 534/600 Iteration: 533 Training loss: 0.57407\n",
      "Epoch: 535/600 Iteration: 534 Training loss: 0.57505\n",
      "Epoch: 534/600 Iteration: 535 Validation Acc: 0.4913\n",
      "Epoch: 536/600 Iteration: 535 Training loss: 0.57042\n",
      "Epoch: 537/600 Iteration: 536 Training loss: 0.57241\n",
      "Epoch: 538/600 Iteration: 537 Training loss: 0.57076\n",
      "Epoch: 539/600 Iteration: 538 Training loss: 0.56581\n",
      "Epoch: 540/600 Iteration: 539 Training loss: 0.56537\n",
      "Epoch: 539/600 Iteration: 540 Validation Acc: 0.4911\n",
      "Epoch: 541/600 Iteration: 540 Training loss: 0.56505\n",
      "Epoch: 542/600 Iteration: 541 Training loss: 0.56146\n",
      "Epoch: 543/600 Iteration: 542 Training loss: 0.56211\n",
      "Epoch: 544/600 Iteration: 543 Training loss: 0.55945\n",
      "Epoch: 545/600 Iteration: 544 Training loss: 0.55618\n",
      "Epoch: 544/600 Iteration: 545 Validation Acc: 0.4945\n",
      "Epoch: 546/600 Iteration: 545 Training loss: 0.55685\n",
      "Epoch: 547/600 Iteration: 546 Training loss: 0.55200\n",
      "Epoch: 548/600 Iteration: 547 Training loss: 0.55269\n",
      "Epoch: 549/600 Iteration: 548 Training loss: 0.55140\n",
      "Epoch: 550/600 Iteration: 549 Training loss: 0.55120\n",
      "Epoch: 549/600 Iteration: 550 Validation Acc: 0.4947\n",
      "Epoch: 551/600 Iteration: 550 Training loss: 0.54613\n",
      "Epoch: 552/600 Iteration: 551 Training loss: 0.55104\n",
      "Epoch: 553/600 Iteration: 552 Training loss: 0.54291\n",
      "Epoch: 554/600 Iteration: 553 Training loss: 0.54635\n",
      "Epoch: 555/600 Iteration: 554 Training loss: 0.53991\n",
      "Epoch: 554/600 Iteration: 555 Validation Acc: 0.4963\n",
      "Epoch: 556/600 Iteration: 555 Training loss: 0.54106\n",
      "Epoch: 557/600 Iteration: 556 Training loss: 0.53607\n",
      "Epoch: 558/600 Iteration: 557 Training loss: 0.53492\n",
      "Epoch: 559/600 Iteration: 558 Training loss: 0.53521\n",
      "Epoch: 560/600 Iteration: 559 Training loss: 0.52981\n",
      "Epoch: 559/600 Iteration: 560 Validation Acc: 0.4974\n",
      "Epoch: 561/600 Iteration: 560 Training loss: 0.53177\n",
      "Epoch: 562/600 Iteration: 561 Training loss: 0.52576\n",
      "Epoch: 563/600 Iteration: 562 Training loss: 0.52783\n",
      "Epoch: 564/600 Iteration: 563 Training loss: 0.52304\n",
      "Epoch: 565/600 Iteration: 564 Training loss: 0.52409\n",
      "Epoch: 564/600 Iteration: 565 Validation Acc: 0.5009\n",
      "Epoch: 566/600 Iteration: 565 Training loss: 0.52034\n",
      "Epoch: 567/600 Iteration: 566 Training loss: 0.52014\n",
      "Epoch: 568/600 Iteration: 567 Training loss: 0.51702\n",
      "Epoch: 569/600 Iteration: 568 Training loss: 0.51745\n",
      "Epoch: 570/600 Iteration: 569 Training loss: 0.51410\n",
      "Epoch: 569/600 Iteration: 570 Validation Acc: 0.4999\n",
      "Epoch: 571/600 Iteration: 570 Training loss: 0.51463\n",
      "Epoch: 572/600 Iteration: 571 Training loss: 0.51231\n",
      "Epoch: 573/600 Iteration: 572 Training loss: 0.51446\n",
      "Epoch: 574/600 Iteration: 573 Training loss: 0.50835\n",
      "Epoch: 575/600 Iteration: 574 Training loss: 0.51094\n",
      "Epoch: 574/600 Iteration: 575 Validation Acc: 0.5043\n",
      "Epoch: 576/600 Iteration: 575 Training loss: 0.50605\n",
      "Epoch: 577/600 Iteration: 576 Training loss: 0.50447\n",
      "Epoch: 578/600 Iteration: 577 Training loss: 0.50063\n",
      "Epoch: 579/600 Iteration: 578 Training loss: 0.50262\n",
      "Epoch: 580/600 Iteration: 579 Training loss: 0.49944\n",
      "Epoch: 579/600 Iteration: 580 Validation Acc: 0.5062\n",
      "Epoch: 581/600 Iteration: 580 Training loss: 0.49769\n",
      "Epoch: 582/600 Iteration: 581 Training loss: 0.49854\n",
      "Epoch: 583/600 Iteration: 582 Training loss: 0.49429\n",
      "Epoch: 584/600 Iteration: 583 Training loss: 0.49194\n",
      "Epoch: 585/600 Iteration: 584 Training loss: 0.49085\n",
      "Epoch: 584/600 Iteration: 585 Validation Acc: 0.5049\n",
      "Epoch: 586/600 Iteration: 585 Training loss: 0.48797\n",
      "Epoch: 587/600 Iteration: 586 Training loss: 0.48774\n",
      "Epoch: 588/600 Iteration: 587 Training loss: 0.48608\n",
      "Epoch: 589/600 Iteration: 588 Training loss: 0.48331\n",
      "Epoch: 590/600 Iteration: 589 Training loss: 0.48231\n",
      "Epoch: 589/600 Iteration: 590 Validation Acc: 0.5055\n",
      "Epoch: 591/600 Iteration: 590 Training loss: 0.48080\n",
      "Epoch: 592/600 Iteration: 591 Training loss: 0.47905\n",
      "Epoch: 593/600 Iteration: 592 Training loss: 0.47790\n",
      "Epoch: 594/600 Iteration: 593 Training loss: 0.47624\n",
      "Epoch: 595/600 Iteration: 594 Training loss: 0.47440\n",
      "Epoch: 594/600 Iteration: 595 Validation Acc: 0.5076\n",
      "Epoch: 596/600 Iteration: 595 Training loss: 0.47331\n",
      "Epoch: 597/600 Iteration: 596 Training loss: 0.47132\n",
      "Epoch: 598/600 Iteration: 597 Training loss: 0.47059\n",
      "Epoch: 599/600 Iteration: 598 Training loss: 0.46885\n",
      "Epoch: 600/600 Iteration: 599 Training loss: 0.46879\n",
      "Epoch: 599/600 Iteration: 600 Validation Acc: 0.5101\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "epochs = 600\n",
    "batch_splits = 1\n",
    "timestamp = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "\n",
    "run_name = \"c%d_b%d_e%d_i%d_t%.1f_%s\" % (classification_id, batch_splits, epochs, im_count_class, test_size, timestamp)\n",
    "print (\"Run name: %s\" % run_name)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "iteration = 0\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter('summaries/%s/train' % run_name, sess.graph)\n",
    "    valid_writer = tf.summary.FileWriter('summaries/%s/valid' % run_name, sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for x, y in get_batches(train_x, train_y, batch_splits):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y}\n",
    "            #loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            summary, loss, _ = sess.run([merged, cost, optimizer], feed_dict=feed)\n",
    "            train_writer.add_summary(summary, iteration)\n",
    "            train_writer.flush()\n",
    "            \n",
    "            print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "                  \"Iteration: {}\".format(iteration),\n",
    "                  \"Training loss: {:.5f}\".format(loss))\n",
    "            iteration += 1\n",
    "            \n",
    "            if iteration % 5 == 0:\n",
    "                feed = {inputs_: val_x,\n",
    "                        labels_: val_y}\n",
    "                #val_acc, = sess.run([accuracy], feed_dict=feed)  \n",
    "                summary, val_acc = sess.run([merged, accuracy], feed_dict=feed)                \n",
    "                valid_writer.add_summary(summary, iteration)\n",
    "                valid_writer.flush()\n",
    "                \n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Validation Acc: {:.4f}\".format(val_acc))\n",
    "                \n",
    "            if iteration % 50 == 0:\n",
    "                saver.save(sess, \"checkpoints/flowers.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Testing\n",
    "\n",
    "Below you see the test accuracy. You can also see the predictions returned for images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    feed = {inputs_: test_x,\n",
    "            labels_: test_y}\n",
    "    test_acc = sess.run(accuracy, feed_dict=feed)\n",
    "    print(\"Test accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import imread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Below, feel free to choose images and see how the trained classifier predicts the flowers in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_img_path = 'food_images/cid12_max2048/6/2294.jpg'\n",
    "test_img = imread(test_img_path)\n",
    "plt.imshow(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Run this cell if you don't have a vgg graph built\n",
    "with tf.Session() as sess:\n",
    "    input_ = tf.placeholder(tf.float32, [None, 224, 224, 3], name='input')\n",
    "    vgg = vgg16.Vgg16()\n",
    "    vgg.build(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    img = utils.load_image(test_img_path)\n",
    "    img = img.reshape((1, 224, 224, 3))\n",
    "\n",
    "    feed_dict = {input_: img}\n",
    "    code = sess.run(vgg.relu6, feed_dict=feed_dict)\n",
    "        \n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    feed = {inputs_: code}\n",
    "    prediction = sess.run(predicted, feed_dict=feed).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:         \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    graph = tf.get_default_graph() \n",
    "    \n",
    "    img = utils.load_image(test_img_path)\n",
    "    img = img.reshape((1, 224, 224, 3))\n",
    "    \n",
    "    image_buffer_input = graph.get_tensor_by_name('input:0')\n",
    "    \n",
    "    feed_dict = {image_buffer_input: img}\n",
    "    prediction = sess.run(predicted, feed_dict=feed_dict).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.barh(np.arange(len(lb.classes_)), prediction)\n",
    "_ = plt.yticks(np.arange(len(lb.classes_)), lb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    feed = {inputs_: test_x,\n",
    "            labels_: test_y}\n",
    "    predictions = sess.run(predicted, feed_dict=feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print (predictions.shape)\n",
    "print(labels[test_idx])\n",
    "print(np.argmax(test_y, 1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Ref: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(np.argmax(test_y, 1), np.argmax(predictions, 1))\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plot_confusion_matrix(cnf_matrix, classes=lb.classes_, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Save graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    #tf.train.write_graph(sess.graph_def, \"./\", \"vgg16_food_transfer_learning.pb\", False)\n",
    "    output_node_names = ['final_result', \"final_training_ops/BiasAdd\"]\n",
    "    \n",
    "    output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "        sess, # The session is used to retrieve the weights\n",
    "        tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes \n",
    "        output_node_names # The output node names are used to select the usefull nodes\n",
    "    ) \n",
    "\n",
    "    # Finally we serialize and dump the output graph to the filesystem\n",
    "    with tf.gfile.GFile(\"vgg16_food_transfer_learning.pb\", \"wb\") as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "    print(\"%d ops in the final graph.\" % len(output_graph_def.node))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
